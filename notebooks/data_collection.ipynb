{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Sentiment Analysis - Reddit vs the News\n",
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created By: Ben Chamblee - https://github.com/Bench-amblee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "- [Introduction](#introduction)\n",
    "- [Imports](#Imports)\n",
    "- [Define Data and Output](#define-data-and-output)\n",
    "- [Reddit Data Collection](#reddit-data-collection)\n",
    "- [News API Data Collection](#news-api-data-collection)\n",
    "- [Combined Data Collection](#combined-data-collection)\n",
    "- [Conclusion and Next Steps](#conclusion-and-next-steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook documents the data collection process for this stock sentiment analysis machine learning project. It encompasses the methodologies, tools, and techniques used to gather the raw data for further analysis and modeling.\n",
    "\n",
    "The primary objectives of this notebook are to:\n",
    "\n",
    "- Collect source data from Reddit and News Sources using APIs\n",
    "- Collect posts and news sources to gather content on the 'Magnificent 7' Stocks (Google, Amazon, Apple, Meta, Microsoft, Nvidia, Tesla)\n",
    "- Document data sources and retrieval methods\n",
    "- Demonstrate the ETL (Extract, Transform, Load) pipeline\n",
    "- Outline quality assurance checks implemented during collection\n",
    "- Provide a clear path for reproducing the data collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw # Reddit API wrapper for Python\n",
    "import re # Regular expressions library for text pattern matching and manipulation\n",
    "import requests # HTTP library for making requests to websites and APIs\n",
    "import pandas as pd # Data manipulation library providing DataFrame structures\n",
    "from datetime import datetime, timedelta # For working with dates and times\n",
    "import time # Provides various time-related functions\n",
    "import os # For interacting with the operating system (file paths, environment variables)\n",
    "from dotenv import load_dotenv # For loading environment variables from a .env file (useful for API keys/credentials)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data and Output\n",
    "\n",
    "Since we are searching for stocks by company name and by ticker, we'll use a dictionary to store both values. We will also create a smaller dictionary for testing purposes and define our output directory for data to be stored.\n",
    "\n",
    "clean_text will be used to save memory - many web pages will return blank space (especially from news pages), this function will remove the blank spaces and only return useful text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Magnificent 7 stocks\n",
    "MAGNIFICENT_7 = {\n",
    "    'GOOGL': 'Alphabet',\n",
    "    'AMZN': 'Amazon',\n",
    "    'AAPL': 'Apple',\n",
    "    'META': 'Meta Platforms',\n",
    "    'MSFT': 'Microsoft',\n",
    "    'NVDA': 'Nvidia',\n",
    "    'TSLA': 'Tesla'\n",
    "}\n",
    "\n",
    "test_dict = {'GOOGL': 'Alphabet'}  # Test with one stock for quick results\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"magnificent7_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Helper function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing extra whitespace and ensuring complete sentences\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit Data Collection\n",
    "\n",
    "collect_reddit_data gathers posts about specific stocks from multiple financial subreddits. It:\n",
    "\n",
    "1. Authenticates with the Reddit API using .env variables (API Key, Client Info)\n",
    "2. Searches across four financial subreddits - r/wallstreetbets, r/stocks, r/investing, and r/StockMarket\n",
    "3. For each stock, perform separate searches and collect posts\n",
    "4. Collect post details including title, body, author, and timestamp\n",
    "5. Retrieves up to 5 top-level comments for each post\n",
    "6. Processes and cleans the text content\n",
    "7. Adds metadata like stock symbol, search term, and collection time\n",
    "8. Compiles everything into a pandas DataFrame, removing duplicates\n",
    "9. Includes error handling and rate limiting (2-second pause between searches)\n",
    "\n",
    "This function returns a dataset of Reddit posts about the specified stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_reddit_data(stocks_dict, post_limit=30):\n",
    "    \"\"\"Collect Reddit data for multiple stocks\"\"\"\n",
    "\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv(\"reddit-CLIENT_ID\"),\n",
    "        client_secret=os.getenv(\"reddit-CLIENT_SECRET\"),\n",
    "        user_agent=os.getenv(\"reddit-USER_AGENT\"),\n",
    "    )\n",
    "    \n",
    "    all_posts_data = []\n",
    "    \n",
    "    # Subreddits to search in\n",
    "    subreddits = [\"wallstreetbets\", \"stocks\", \"investing\", \"StockMarket\"]\n",
    "    subreddit_obj = reddit.subreddit(\"+\".join(subreddits))\n",
    "    \n",
    "    # Collect data for each stock\n",
    "    for symbol, company_name in stocks_dict.items():\n",
    "        print(f\"Collecting Reddit data for {company_name} ({symbol})...\")\n",
    "        \n",
    "        # Search terms - include both ticker and company name\n",
    "        search_terms = [symbol, company_name]\n",
    "        \n",
    "        for search_term in search_terms:\n",
    "            print(f\"  - Searching for '{search_term}'\")\n",
    "            \n",
    "            try:\n",
    "                # Search for posts containing the term\n",
    "                posts = subreddit_obj.search(search_term, limit=post_limit, sort=\"relevance\")\n",
    "                \n",
    "                post_count = 0\n",
    "                for post in posts:\n",
    "                    try:\n",
    "                        # Get the subreddit name\n",
    "                        sub_name = post.subreddit.display_name\n",
    "                        \n",
    "                        # Get top-level comments (limited to 5 for simplicity)\n",
    "                        try:\n",
    "                            post.comments.replace_more(limit=0)\n",
    "                            comments = []\n",
    "                            for comment in list(post.comments)[:5]:\n",
    "                                cleaned_comment = clean_text(comment.body)\n",
    "                                if cleaned_comment:  # Only add non-empty comments\n",
    "                                    comments.append(cleaned_comment)\n",
    "                        except Exception as e:\n",
    "                            print(f\"    Error fetching comments: {e}\")\n",
    "                            comments = []\n",
    "                        \n",
    "                        # Convert created UTC to readable time\n",
    "                        created_time = datetime.fromtimestamp(post.created_utc)\n",
    "                        \n",
    "                        # Clean and prepare the post body\n",
    "                        body = clean_text(post.selftext)\n",
    "                        title = clean_text(post.title)\n",
    "                        \n",
    "                        # Store data with stock information\n",
    "                        post_data = {\n",
    "                            \"platform\": \"Reddit\",\n",
    "                            \"stock_symbol\": symbol,\n",
    "                            \"stock_name\": company_name,\n",
    "                            \"post_id\": post.id,\n",
    "                            \"title\": title,\n",
    "                            \"body\": body,\n",
    "                            \"author\": str(post.author),\n",
    "                            \"score\": post.score,\n",
    "                            \"created_at\": created_time,\n",
    "                            \"num_comments\": post.num_comments,\n",
    "                            \"comments\": str(comments),  # Convert list to string for CSV\n",
    "                            \"subreddit\": sub_name,\n",
    "                            \"url\": post.url,\n",
    "                            \"search_term\": search_term,\n",
    "                            \"collection_time\": datetime.now()\n",
    "                        }\n",
    "                        all_posts_data.append(post_data)\n",
    "                        post_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"    Error processing post: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"    Found {post_count} posts for '{search_term}'\")\n",
    "                \n",
    "                # Sleep to respect rate limits\n",
    "                time.sleep(2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during Reddit search for {search_term}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_posts_data)\n",
    "    \n",
    "    # Remove any duplicates based on post_id\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=['post_id'])\n",
    "    \n",
    "    print(f\"Total Reddit posts collected: {len(df)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function and take a look at the reddit data to make sure it all looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Reddit data for Alphabet (GOOGL)...\n",
      "  - Searching for 'GOOGL'\n",
      "    Found 30 posts for 'GOOGL'\n",
      "  - Searching for 'Alphabet'\n",
      "    Found 30 posts for 'Alphabet'\n",
      "Total Reddit posts collected: 59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>stock_symbol</th>\n",
       "      <th>stock_name</th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>created_at</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>search_term</th>\n",
       "      <th>collection_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet</td>\n",
       "      <td>1j739da</td>\n",
       "      <td>GOOGL is the most bullish \"safe\" stock for lon...</td>\n",
       "      <td>My arguments are the following: \\- Alphabet ha...</td>\n",
       "      <td>SwissCowOnMoon</td>\n",
       "      <td>657</td>\n",
       "      <td>2025-03-09 04:28:35</td>\n",
       "      <td>381</td>\n",
       "      <td>['If they get broken up (whether by force or c...</td>\n",
       "      <td>stocks</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j739...</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2025-03-20 10:00:19.176094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet</td>\n",
       "      <td>1izuaqk</td>\n",
       "      <td>Is Google ($GOOGL) a great long-term buy after...</td>\n",
       "      <td>Google (GOOGL) has been dropping quite a bit r...</td>\n",
       "      <td>biznisgod</td>\n",
       "      <td>329</td>\n",
       "      <td>2025-02-27 18:56:30</td>\n",
       "      <td>222</td>\n",
       "      <td>['The drop is broad based across all the mega-...</td>\n",
       "      <td>investing</td>\n",
       "      <td>https://www.reddit.com/r/investing/comments/1i...</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2025-03-20 10:00:20.468976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet</td>\n",
       "      <td>1je3bod</td>\n",
       "      <td>$GOOG &amp; $GOOGL buy WIZ start up $32 Billion</td>\n",
       "      <td>$GOOG &amp; $GOOGL buy WIZ start up $32 Billion [h...</td>\n",
       "      <td>Othe-un-dots</td>\n",
       "      <td>1246</td>\n",
       "      <td>2025-03-18 08:03:22</td>\n",
       "      <td>223</td>\n",
       "      <td>['**User Report**| | | | :--|:--|:--|:-- **Tot...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2025-03-20 10:00:21.530009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet</td>\n",
       "      <td>1j3nziz</td>\n",
       "      <td>Waiting to be stimulated GOOGL</td>\n",
       "      <td>Let's wait for the next move What's going to h...</td>\n",
       "      <td>apslumas</td>\n",
       "      <td>55</td>\n",
       "      <td>2025-03-04 17:47:50</td>\n",
       "      <td>27</td>\n",
       "      <td>['**User Report**| | | | :--|:--|:--|:-- **Tot...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2025-03-20 10:00:21.780409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet</td>\n",
       "      <td>1gwss9g</td>\n",
       "      <td>Why is nobody talking about GOOGL?</td>\n",
       "      <td>Yesterday I was thinking about GOOGL as a safe...</td>\n",
       "      <td>britax12</td>\n",
       "      <td>84</td>\n",
       "      <td>2024-11-21 17:52:06</td>\n",
       "      <td>177</td>\n",
       "      <td>['**User Report**| | | | :--|:--|:--|:-- **Tot...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2025-03-20 10:00:22.643566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  platform stock_symbol stock_name  post_id  \\\n",
       "0   Reddit        GOOGL   Alphabet  1j739da   \n",
       "1   Reddit        GOOGL   Alphabet  1izuaqk   \n",
       "2   Reddit        GOOGL   Alphabet  1je3bod   \n",
       "3   Reddit        GOOGL   Alphabet  1j3nziz   \n",
       "4   Reddit        GOOGL   Alphabet  1gwss9g   \n",
       "\n",
       "                                               title  \\\n",
       "0  GOOGL is the most bullish \"safe\" stock for lon...   \n",
       "1  Is Google ($GOOGL) a great long-term buy after...   \n",
       "2        $GOOG & $GOOGL buy WIZ start up $32 Billion   \n",
       "3                     Waiting to be stimulated GOOGL   \n",
       "4                 Why is nobody talking about GOOGL?   \n",
       "\n",
       "                                                body          author  score  \\\n",
       "0  My arguments are the following: \\- Alphabet ha...  SwissCowOnMoon    657   \n",
       "1  Google (GOOGL) has been dropping quite a bit r...       biznisgod    329   \n",
       "2  $GOOG & $GOOGL buy WIZ start up $32 Billion [h...    Othe-un-dots   1246   \n",
       "3  Let's wait for the next move What's going to h...        apslumas     55   \n",
       "4  Yesterday I was thinking about GOOGL as a safe...        britax12     84   \n",
       "\n",
       "           created_at  num_comments  \\\n",
       "0 2025-03-09 04:28:35           381   \n",
       "1 2025-02-27 18:56:30           222   \n",
       "2 2025-03-18 08:03:22           223   \n",
       "3 2025-03-04 17:47:50            27   \n",
       "4 2024-11-21 17:52:06           177   \n",
       "\n",
       "                                            comments       subreddit  \\\n",
       "0  ['If they get broken up (whether by force or c...          stocks   \n",
       "1  ['The drop is broad based across all the mega-...       investing   \n",
       "2  ['**User Report**| | | | :--|:--|:--|:-- **Tot...  wallstreetbets   \n",
       "3  ['**User Report**| | | | :--|:--|:--|:-- **Tot...  wallstreetbets   \n",
       "4  ['**User Report**| | | | :--|:--|:--|:-- **Tot...  wallstreetbets   \n",
       "\n",
       "                                                 url search_term  \\\n",
       "0  https://www.reddit.com/r/stocks/comments/1j739...       GOOGL   \n",
       "1  https://www.reddit.com/r/investing/comments/1i...       GOOGL   \n",
       "2  https://www.reddit.com/r/wallstreetbets/commen...       GOOGL   \n",
       "3  https://www.reddit.com/r/wallstreetbets/commen...       GOOGL   \n",
       "4  https://www.reddit.com/r/wallstreetbets/commen...       GOOGL   \n",
       "\n",
       "             collection_time  \n",
       "0 2025-03-20 10:00:19.176094  \n",
       "1 2025-03-20 10:00:20.468976  \n",
       "2 2025-03-20 10:00:21.530009  \n",
       "3 2025-03-20 10:00:21.780409  \n",
       "4 2025-03-20 10:00:22.643566  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reddit data test\n",
    "reddit_df = collect_reddit_data(test_dict)\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News API Data Collection\n",
    "\n",
    "collect_newsapi_articles gathers news articles about specific stocks using the NewsAPI service. It:\n",
    "\n",
    "1. Authenticates with NewsAPI using .env variables (API Key)\n",
    "2. Searches for articles published in the last year\n",
    "3. Constructs specific searches for each stock like \"AAPL stock\" and \"Apple earnings\"\n",
    "4. Makes HTTP request to the NewsAPI endpoint\n",
    "5. Processes the JSON response to extract data\n",
    "6. Cleans and normalizes text content (title, description, content)\n",
    "7. Combines description and content for easier analysis\n",
    "8. Captures metadata including source, author, URL, and publication date\n",
    "9. Implements error handling and rate limits (1 second)\n",
    "10. Returns a pandas DataFrame of all collected articles with duplicates removed\n",
    "\n",
    "The function provides a structured daatset of news coverage for each specified stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_newsapi_articles(stocks_dict, api_key=None, max_articles=30):\n",
    "    \"\"\"Collect news articles from NewsAPI for multiple stocks\"\"\"\n",
    "    \n",
    "    all_news_data = []\n",
    "    \n",
    "    # Get API key from parameter or environment variable\n",
    "    if api_key is None:\n",
    "        api_key = os.getenv(\"NEWSAPI_KEY\")\n",
    "    \n",
    "    # Check if API key exists\n",
    "    if not api_key:\n",
    "        print(\"Error: NewsAPI key not found.\")\n",
    "        print(\"Please provide an API key or set the NEWSAPI_KEY environment variable.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calculate date range (365 days ago to today)\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=365)\n",
    "    \n",
    "    # Format dates for API\n",
    "    from_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "    to_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Base URL for NewsAPI\n",
    "    base_url = \"https://newsapi.org/v2/everything\"\n",
    "    \n",
    "    # Headers for API request\n",
    "    headers = {\n",
    "        \"X-Api-Key\": api_key\n",
    "    }\n",
    "    \n",
    "    # Collect news for each stock\n",
    "    for symbol, company_name in stocks_dict.items():\n",
    "        print(f\"Collecting NewsAPI articles for {company_name} ({symbol})...\")\n",
    "        \n",
    "        # Create more specific search queries\n",
    "        queries = [\n",
    "            f\"{symbol} stock\",\n",
    "            f\"{company_name} stock\",\n",
    "            f\"{company_name} earnings\",\n",
    "            f\"{company_name} financial\"\n",
    "        ]\n",
    "        \n",
    "        for query in queries:\n",
    "            print(f\"  - Searching for '{query}'\")\n",
    "            \n",
    "            # Parameters for API request\n",
    "            params = {\n",
    "                \"q\": query,\n",
    "                \"from\": from_date,\n",
    "                \"to\": to_date,\n",
    "                \"language\": \"en\",\n",
    "                \"sortBy\": \"relevancy\",\n",
    "                \"pageSize\": min(max_articles, 100)  # API limit is 100 per request\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # Make API request\n",
    "                response = requests.get(base_url, params=params, headers=headers)\n",
    "                \n",
    "                # Print status for debugging\n",
    "                print(f\"    Status code: {response.status_code}\")\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    \n",
    "                    # Extract articles\n",
    "                    articles = data.get('articles', [])\n",
    "                    print(f\"    Found {len(articles)} articles\")\n",
    "                    \n",
    "                    # Check if there are any articles\n",
    "                    if not articles:\n",
    "                        print(\"    No articles found\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Process each article\n",
    "                    article_count = 0\n",
    "                    for article in articles:\n",
    "                        try:\n",
    "                            # Parse published date\n",
    "                            published_at = article.get('publishedAt', '')\n",
    "                            \n",
    "                            # Clean text fields\n",
    "                            title = clean_text(article.get('title', ''))\n",
    "                            description = clean_text(article.get('description', ''))\n",
    "                            content = clean_text(article.get('content', ''))\n",
    "                            \n",
    "                            # Combine description and content for better sentiment analysis\n",
    "                            full_text = description\n",
    "                            if content and content != description:\n",
    "                                if full_text:\n",
    "                                    full_text += \" \" + content\n",
    "                                else:\n",
    "                                    full_text = content\n",
    "                            \n",
    "                            # Store article data\n",
    "                            article_data = {\n",
    "                                \"platform\": \"NewsAPI\",\n",
    "                                \"stock_symbol\": symbol,\n",
    "                                \"stock_name\": company_name,\n",
    "                                \"title\": title if title else \"No title\",\n",
    "                                \"full_text\": full_text,\n",
    "                                \"source\": article.get('source', {}).get('name', 'Unknown'),\n",
    "                                \"author\": article.get('author', 'Unknown'),\n",
    "                                \"url\": article.get('url', ''),\n",
    "                                \"published_at\": published_at,\n",
    "                                \"search_query\": query,\n",
    "                                \"collection_time\": datetime.now()\n",
    "                            }\n",
    "                            \n",
    "                            # Only add if we have a title and URL\n",
    "                            if article_data[\"title\"] != \"No title\" and article_data[\"url\"]:\n",
    "                                all_news_data.append(article_data)\n",
    "                                article_count += 1\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"    Error processing article: {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    print(f\"    Successfully processed {article_count} articles\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"    Error: API returned status code {response.status_code}\")\n",
    "                    if response.status_code == 401:\n",
    "                        print(\"    Invalid API key or authentication error\")\n",
    "                    elif response.status_code == 429:\n",
    "                        print(\"    Rate limit exceeded\")\n",
    "                    response_text = response.text[:200] + \"...\" if len(response.text) > 200 else response.text\n",
    "                    print(f\"    Response: {response_text}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error collecting NewsAPI data for {query}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Sleep to respect rate limits\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Create DataFrame from the collected data\n",
    "    df = pd.DataFrame(all_news_data)\n",
    "    \n",
    "    # Remove duplicates based on URL\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=['url'])\n",
    "    \n",
    "    print(f\"Total NewsAPI articles collected: {len(df)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function and take a look at a sample to make sure it looks alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting NewsAPI articles for Alphabet (GOOGL)...\n",
      "  - Searching for 'GOOGL stock'\n",
      "    Status code: 200\n",
      "    Found 30 articles\n",
      "    Successfully processed 30 articles\n",
      "  - Searching for 'Alphabet stock'\n",
      "    Status code: 200\n",
      "    Found 29 articles\n",
      "    Successfully processed 29 articles\n",
      "  - Searching for 'Alphabet earnings'\n",
      "    Status code: 200\n",
      "    Found 30 articles\n",
      "    Successfully processed 30 articles\n",
      "  - Searching for 'Alphabet financial'\n",
      "    Status code: 200\n",
      "    Found 29 articles\n",
      "    Successfully processed 29 articles\n",
      "Total NewsAPI articles collected: 82\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>stock_symbol</th>\n",
       "      <th>stock_name</th>\n",
       "      <th>title</th>\n",
       "      <th>full_text</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>published_at</th>\n",
       "      <th>search_query</th>\n",
       "      <th>collection_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet</td>\n",
       "      <td>D-WAVE QUANTUM Stock Rises 62% Post Q4 Results...</td>\n",
       "      <td>QBTS stock benefits from an expanding clientel...</td>\n",
       "      <td>Yahoo Entertainment</td>\n",
       "      <td>Nilanshi Mukherjee</td>\n",
       "      <td>https://finance.yahoo.com/news/d-wave-quantum-...</td>\n",
       "      <td>2025-03-18T17:54:00Z</td>\n",
       "      <td>GOOGL stock</td>\n",
       "      <td>2025-03-20 10:04:08.526688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet</td>\n",
       "      <td>Nvidia, Google, Tesla, BYD, Tencent Music, Xpe...</td>\n",
       "      <td>Stock futures edged down ahead of a slew of ec...</td>\n",
       "      <td>Quartz India</td>\n",
       "      <td>Josh Fellman</td>\n",
       "      <td>https://qz.com/nvidia-google-byd-tesla-xpeng-t...</td>\n",
       "      <td>2025-03-18T12:18:00Z</td>\n",
       "      <td>GOOGL stock</td>\n",
       "      <td>2025-03-20 10:04:08.526688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet</td>\n",
       "      <td>Is Alphabet Inc. (GOOGL) the Most Profitable L...</td>\n",
       "      <td>If you click 'Accept all', we and our partners...</td>\n",
       "      <td>Yahoo Entertainment</td>\n",
       "      <td>None</td>\n",
       "      <td>https://consent.yahoo.com/v2/collectConsent?se...</td>\n",
       "      <td>2025-03-13T23:54:18Z</td>\n",
       "      <td>GOOGL stock</td>\n",
       "      <td>2025-03-20 10:04:08.527798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet</td>\n",
       "      <td>Is Alphabet Inc. (GOOGL) the Top Stock to Buy ...</td>\n",
       "      <td>If you click 'Accept all', we and our partners...</td>\n",
       "      <td>Yahoo Entertainment</td>\n",
       "      <td>None</td>\n",
       "      <td>https://consent.yahoo.com/v2/collectConsent?se...</td>\n",
       "      <td>2025-03-18T22:20:05Z</td>\n",
       "      <td>GOOGL stock</td>\n",
       "      <td>2025-03-20 10:04:08.527798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NewsAPI</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet</td>\n",
       "      <td>Facts About The Stock Corrections, Tariffs, An...</td>\n",
       "      <td>Stocks entered into a correction with a declin...</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Bill Stone, Contributor, \\n Bill Stone, Contri...</td>\n",
       "      <td>https://www.forbes.com/sites/bill_stone/2025/0...</td>\n",
       "      <td>2025-03-16T11:00:00Z</td>\n",
       "      <td>GOOGL stock</td>\n",
       "      <td>2025-03-20 10:04:08.527798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  platform stock_symbol stock_name  \\\n",
       "0  NewsAPI        GOOGL   Alphabet   \n",
       "1  NewsAPI        GOOGL   Alphabet   \n",
       "2  NewsAPI        GOOGL   Alphabet   \n",
       "3  NewsAPI        GOOGL   Alphabet   \n",
       "4  NewsAPI        GOOGL   Alphabet   \n",
       "\n",
       "                                               title  \\\n",
       "0  D-WAVE QUANTUM Stock Rises 62% Post Q4 Results...   \n",
       "1  Nvidia, Google, Tesla, BYD, Tencent Music, Xpe...   \n",
       "2  Is Alphabet Inc. (GOOGL) the Most Profitable L...   \n",
       "3  Is Alphabet Inc. (GOOGL) the Top Stock to Buy ...   \n",
       "4  Facts About The Stock Corrections, Tariffs, An...   \n",
       "\n",
       "                                           full_text               source  \\\n",
       "0  QBTS stock benefits from an expanding clientel...  Yahoo Entertainment   \n",
       "1  Stock futures edged down ahead of a slew of ec...         Quartz India   \n",
       "2  If you click 'Accept all', we and our partners...  Yahoo Entertainment   \n",
       "3  If you click 'Accept all', we and our partners...  Yahoo Entertainment   \n",
       "4  Stocks entered into a correction with a declin...               Forbes   \n",
       "\n",
       "                                              author  \\\n",
       "0                                 Nilanshi Mukherjee   \n",
       "1                                       Josh Fellman   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4  Bill Stone, Contributor, \\n Bill Stone, Contri...   \n",
       "\n",
       "                                                 url          published_at  \\\n",
       "0  https://finance.yahoo.com/news/d-wave-quantum-...  2025-03-18T17:54:00Z   \n",
       "1  https://qz.com/nvidia-google-byd-tesla-xpeng-t...  2025-03-18T12:18:00Z   \n",
       "2  https://consent.yahoo.com/v2/collectConsent?se...  2025-03-13T23:54:18Z   \n",
       "3  https://consent.yahoo.com/v2/collectConsent?se...  2025-03-18T22:20:05Z   \n",
       "4  https://www.forbes.com/sites/bill_stone/2025/0...  2025-03-16T11:00:00Z   \n",
       "\n",
       "  search_query            collection_time  \n",
       "0  GOOGL stock 2025-03-20 10:04:08.526688  \n",
       "1  GOOGL stock 2025-03-20 10:04:08.526688  \n",
       "2  GOOGL stock 2025-03-20 10:04:08.527798  \n",
       "3  GOOGL stock 2025-03-20 10:04:08.527798  \n",
       "4  GOOGL stock 2025-03-20 10:04:08.527798  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# news api data test\n",
    "news_df = collect_newsapi_articles(test_dict)\n",
    "news_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Data Collection\n",
    "\n",
    "combined_data_collection runs both of the previous data collection scripts for all 7 of the magnificent 7 stocks. In total it should take ~20 minutes to gather all the data and output in 3 datasets: one for just reddit, one for just newsAPI, and one combined. The data will be sent to output directory that was defined in the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_data_collection():\n",
    "    \"\"\"Main function to collect data from Reddit and NewsAPI for Magnificent 7 stocks\"\"\"\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    all_data_frames = []\n",
    "    \n",
    "    try:\n",
    "        # Reddit data\n",
    "        reddit_df = collect_reddit_data(MAGNIFICENT_7)\n",
    "        if not reddit_df.empty:\n",
    "            reddit_df.to_csv(f\"{output_dir}/reddit_mag7_{timestamp}.csv\", index=False, encoding='utf-8')\n",
    "            all_data_frames.append(reddit_df)\n",
    "        \n",
    "        # NewsAPI data\n",
    "        newsapi_df = collect_newsapi_articles(MAGNIFICENT_7)\n",
    "        if not newsapi_df.empty:\n",
    "            newsapi_df.to_csv(f\"{output_dir}/newsapi_mag7_{timestamp}.csv\", index=False, encoding='utf-8')\n",
    "            all_data_frames.append(newsapi_df)\n",
    "        \n",
    "        # Combine all data into one DataFrame\n",
    "        if all_data_frames:\n",
    "            combined_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "            \n",
    "            # Save combined data\n",
    "            combined_file = f\"{output_dir}/magnificent7_combined_{timestamp}.csv\"\n",
    "            combined_df.to_csv(combined_file, index=False, encoding='utf-8')\n",
    "            print(f\"\\nCombined data saved to {combined_file}\")\n",
    "            print(f\"Total collected items: {len(combined_df)}\")\n",
    "            \n",
    "            # Print data distribution\n",
    "            print(\"\\nData Distribution:\")\n",
    "            platform_counts = combined_df['platform'].value_counts()\n",
    "            for platform, count in platform_counts.items():\n",
    "                print(f\"  {platform}: {count} items\")\n",
    "                \n",
    "            stock_counts = combined_df['stock_symbol'].value_counts()\n",
    "            print(\"\\nData by Stock:\")\n",
    "            for stock, count in stock_counts.items():\n",
    "                print(f\"  {stock}: {count} items\")\n",
    "        else:\n",
    "            print(\"\\nNo data was collected from any source.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data collection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run combined data collection function\n",
    "combined_data_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Next Steps\n",
    "\n",
    "This notebook successfully establishes a data collection pipeline for financial market analysis. We've implemented two primary data sources:\n",
    "\n",
    "Reddit Data: Using praw to gather social media sentiment from investment communities including r/wallstreetbets, r/stocks, r/investing, and r/StockMarket.\n",
    "News Articles: Using NewsAPI to collect recent financial news about our target stocks from reputable sources.\n",
    "\n",
    "The collection process includes proper error handling, rate limiting, and deduplication to ensure data quality. This foundation provides us with a diverse dataset that captures both institutional perspectives (news) and retail investor sentiment (Reddit).\n",
    "\n",
    "#### Next Steps\n",
    "\n",
    "Data Cleaning & Exploratory Data Analysis (EDA)\n",
    "\n",
    "- Standardize text formats and remove special characters\n",
    "- Handle missing values appropriately\n",
    "- Convert timestamps to a consistent format\n",
    "- Extract meaningful features from raw text\n",
    "- Analyze post/article frequency by stock and source\n",
    "- Assess initial sentiment distribution\n",
    "- Tokenize and normalize text data\n",
    "- Remove stopwords and irrelevant content\n",
    "\n",
    "The cleaned dataset and insights from our EDA will form the foundation for our sentiment analysis models, which will help us quantify market sentiment and potentially identify correlations with stock price movements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
